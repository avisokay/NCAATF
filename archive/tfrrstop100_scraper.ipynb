{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping https://www.tfrrs.org/ Performance List Archives \n",
    "By Adam Visokay\n",
    "### This scraper will gather available data for Outdoor Qualifying lists from 2010-2019 and 2021 for Divisions I, II and III.  I ignore 2020 because those performance lists are incomplete due to the COVID-19 pandemic ending the season prematurely in March.  \n",
    "### Each year and division has it's own url.  \n",
    "### Each url contains the top 100 performances at the end of the regular season (before championships) for each of the following NCAA Outdoor Championship events :\n",
    "### 100 200 400 800 1500 5000 10000 100H 110H 400H 3000S 4x100 4x400 HJ PV LJ TJ SP DT HT JT Hep Dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate functions I will use to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_events(df_list):\n",
    "    '''Add EVENT column to each df in df_list from event list.\n",
    "    Alternating sexes starting with Men for each event, except 100/110H and Hep/Dec are reversed.'''\n",
    "    \n",
    "    events = ['100', '100', '200', '200', '400', '400', '800', '800', '1500', '1500', '5000', '5000', '10000', '10000', \n",
    "              '100H', '110H', '400H', '400H', '3000S', '3000S', '4x100', '4x100', '4x400', '4x400', \n",
    "              'HJ', 'HJ', 'PV', 'PV', 'LJ', 'LJ', 'TJ', 'TJ', 'SP', 'SP', 'DT', 'DT', 'HT', 'HT', 'JT', 'JT', \n",
    "              'Hep', 'Dec']\n",
    "    \n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i]['EVENT'] = events[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sex(df_list):\n",
    "    '''Add event to SEX column (Men/Women) to each df in df_list from event list.\n",
    "    Male recorded first in df for each event except 100/110H and Hep/Dec.'''\n",
    "    \n",
    "    for i in range(14):\n",
    "        if i%2 == 0:\n",
    "            df_list[i]['SEX'] = 'Men'\n",
    "        else:\n",
    "            df_list[i]['SEX'] = 'Women'\n",
    "    df_list[14]['SEX'] = 'Women'\n",
    "    df_list[15]['SEX'] = 'Men'\n",
    "    for i in range(16,40):\n",
    "        if i%2 == 0:\n",
    "            df_list[i]['SEX'] = 'Men'\n",
    "        else:\n",
    "            df_list[i]['SEX'] = 'Women'\n",
    "    df_list[40]['SEX'] = 'Women'\n",
    "    df_list[41]['SEX'] = 'Men'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_division(df_list, division):\n",
    "    '''Add DIVISION column (D1, D2 or D3) to each df in df_list from input parameter division.''' \n",
    "    \n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i]['DIVISION'] = division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_champ_year(df_list):\n",
    "    '''Add CHAMP_YEAR column to each df in df_list from last 4 chars from df['MEET DATE'] column.'''\n",
    "    \n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i]['CHAMP_YEAR'] = [j[-4:] for j in df_list[i]['MEET DATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_time(df_list):\n",
    "    '''Takes time in format %M:%S.%f or %S.%f and converts to %M:%S:%f or %S:%f for parsing with datetime library.'''\n",
    "    \n",
    "    for df in df_list:\n",
    "        if ~df['TIME'].astype(str).str.contains('nan').any():\n",
    "            df['TIME'] = df['TIME'].astype(str) # make sure all times are str format\n",
    "            df['TIME'] = [re.sub(re.compile(r'\\([^)]*\\)'), '', i) for i in df['TIME']]\n",
    "            df['TIME'] = df['TIME'].str.replace('.',':').str.replace('@','').str.replace('h','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_total_seconds(df_list):\n",
    "    '''Takes dfs from df_list that have had TIME cleaned and creates a TIME_SECS column of total seconds.'''\n",
    "    \n",
    "    for df in df_list:\n",
    "        if ~df['TIME'].astype(str).str.contains('nan').any():\n",
    "            results = []\n",
    "            for time in df['TIME']:\n",
    "                if len(time) <6:\n",
    "                    date_time = datetime.datetime.strptime(time, '%S:%f')\n",
    "                    a_timedelta = date_time - datetime.datetime(1900, 1, 1)\n",
    "                    results.append(a_timedelta.total_seconds())\n",
    "                else:\n",
    "                    date_time = datetime.datetime.strptime(time, \"%M:%S:%f\")\n",
    "                    a_timedelta = date_time - datetime.datetime(1900, 1, 1)\n",
    "                    results.append(a_timedelta.total_seconds())\n",
    "            df['TIME_SECS'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine above functions into one cleaning function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dfs(df_list, division):\n",
    "    '''Take in list of dataframes, and division(str).\n",
    "       Loop through each df in df_list and add column for:\n",
    "       event, sex, division and championship year.\n",
    "       Reorder rename and drop columns appropriately.\n",
    "       Add TIME_SECS from TIME column for running events.\n",
    "       '''\n",
    "    \n",
    "    add_events(df_list)\n",
    "    add_sex(df_list)\n",
    "    add_division(df_list, division)\n",
    "    add_champ_year(df_list)\n",
    "    \n",
    "    # rename column to POSITION\n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i].rename(columns={'Unnamed: 0' : 'POSITION'}, inplace=True)\n",
    "    \n",
    "    # reorder and drop columns\n",
    "    col_names = ['POSITION', 'CHAMP_YEAR', 'DIVISION', 'EVENT', 'SEX', 'ATHLETE', 'YEAR', 'TEAM', 'TIME', 'TIME_SECS', \n",
    "                 'MARK', 'CONV', 'POINTS', 'MEET', 'MEET DATE']    \n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i] = df_list[i].reindex(columns=col_names)\n",
    "        \n",
    "    \n",
    "    clean_time(df_list)\n",
    "    add_total_seconds(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final function that loops through list of urls for each division, scrapes and then cleans them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tfrrs(url_list, division):\n",
    "    '''For each url in the list, clean the dfs and return a concatenated version of all dfs'''\n",
    "    cleaned_dfs = []\n",
    "    for url in url_list:\n",
    "        # scrape url into list of dfs\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage)\n",
    "        df_list = pd.read_html(webpage)\n",
    "        \n",
    "        # clean each df in df_list\n",
    "        clean_dfs(df_list, division)\n",
    "        \n",
    "        # add each cleaned df to cleaned_dfs list\n",
    "        for i in df_list:\n",
    "            cleaned_dfs.append(i)\n",
    "        \n",
    "    # concatenate cleaned_dfs list into one long dataframe\n",
    "    result = pd.concat(cleaned_dfs)\n",
    "    result.index = range(1, len(result) + 1)\n",
    "    \n",
    "    return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEAN ONE URL AT A TIME\n",
    "Unfortunately the D1 2010 url includes a Men's and Women's 3000m which throws off the scraper, So D1 2010 must be handled on it's own. This is a function modified to include the same code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_variables(df_list, division):\n",
    "    '''Cleaning function using modified version of above code to appropriately clean and modify dfs in df_list'''\n",
    "    \n",
    "    # EVENT\n",
    "    events = ['100', '100', '200', '200', '400', '400', '800', '800', '1500', '1500', '5000', '5000', '10000', '10000', \n",
    "              '100H', '110H', '400H', '400H', '3000S', '3000S', '4x100', '4x100', '4x400', '4x400', \n",
    "              'HJ', 'HJ', 'PV', 'PV', 'LJ', 'LJ', 'TJ', 'TJ', 'SP', 'SP', 'DT', 'DT', 'HT', 'HT', 'JT', 'JT', \n",
    "              'Hep', 'Dec']    \n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i]['EVENT'] = events[i]\n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i]['EVENT'] = events[i]\n",
    "        \n",
    "    # SEX\n",
    "    for i in range(14):\n",
    "        if i%2 == 0:\n",
    "            df_list[i]['SEX'] = 'Men'\n",
    "        else:\n",
    "            df_list[i]['SEX'] = 'Women'\n",
    "    df_list[14]['SEX'] = 'Women'\n",
    "    df_list[15]['SEX'] = 'Men'\n",
    "    for i in range(16,40):\n",
    "        if i%2 == 0:\n",
    "            df_list[i]['SEX'] = 'Men'\n",
    "        else:\n",
    "            df_list[i]['SEX'] = 'Women'\n",
    "    df_list[40]['SEX'] = 'Women'\n",
    "    df_list[41]['SEX'] = 'Men'\n",
    "\n",
    "    # DIVISION\n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i]['DIVISION'] = division\n",
    "        \n",
    "    # CHAMP_YEAR\n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i]['CHAMP_YEAR'] = [j[-4:] for j in df_list[i]['MEET DATE']]\n",
    "        \n",
    "    # rename column to POSITION\n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i].rename(columns={'Unnamed: 0' : 'POSITION'}, inplace=True)\n",
    "    \n",
    "    # reorder and drop columns\n",
    "    col_names = ['POSITION', 'CHAMP_YEAR', 'DIVISION', 'EVENT', 'SEX', 'ATHLETE', 'YEAR', 'TEAM', 'TIME', 'TIME_SECS', \n",
    "                 'MARK', 'CONV', 'POINTS', 'MEET', 'MEET DATE']    \n",
    "    for i in range(len(df_list)):\n",
    "        df_list[i] = df_list[i].reindex(columns=col_names)\n",
    "        \n",
    "    # clean time\n",
    "    for df in df_list:\n",
    "        if ~df['TIME'].astype(str).str.contains('nan').any():\n",
    "            df['TIME'] = df['TIME'].astype(str) # make sure all times are str format\n",
    "            df['TIME'] = [re.sub(re.compile(r'\\([^)]*\\)'), '', i) for i in df['TIME']]\n",
    "            df['TIME'] = df['TIME'].str.replace('.',':').str.replace('@','').str.replace('h','')\n",
    "            \n",
    "    # TIME_SECS\n",
    "    for df in df_list:\n",
    "        if ~df['TIME'].astype(str).str.contains('nan').any():\n",
    "            results = []\n",
    "            for time in df['TIME']:\n",
    "                if len(time) <6:\n",
    "                    date_time = datetime.datetime.strptime(time, '%S:%f')\n",
    "                    a_timedelta = date_time - datetime.datetime(1900, 1, 1)\n",
    "                    results.append(a_timedelta.total_seconds())\n",
    "                else:\n",
    "                    date_time = datetime.datetime.strptime(time, \"%M:%S:%f\")\n",
    "                    a_timedelta = date_time - datetime.datetime(1900, 1, 1)\n",
    "                    results.append(a_timedelta.total_seconds())\n",
    "            df['TIME_SECS'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAPE ONE URL AT A TIME\n",
    "Scrape the D1 2010 url into a list of dataframes for each event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "d12010 = 'https://www.tfrrs.org/lists/528.html'\n",
    "\n",
    "req = Request(d12010, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "soup = BeautifulSoup(webpage)\n",
    "dfs = pd.read_html(webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deep copy of dfs to work with\n",
    "dfs_copy2010 = [i.copy(deep=True) for i in dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This shows the top results for the Men's and Women's 3000m in the 2010 results that we do not want in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_copy2010[10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_copy2010[11].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the 3000m results from the list of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfs_copy2010[10]\n",
    "del dfs_copy2010[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it correctly skips from Women's 1500 to Men's 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_copy2010[9].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_copy2010[10].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the cleaning function from above, let's clean the 2010 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_variables(dfs_copy2010, 'D1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a peek at the Women's 10000, it looks like we have what we want for our edge case DI 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_copy2010[13].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of URLS for each Division I and Division II\n",
    "Now that we have handled the edge case DI 2010, we can use the automated looping scraper to handle sets of urls at a time for each division I, II, and III. This list of urls is taken from https://www.tfrrs.org/archives.html and includes the top 100 performances per event from the available years 2010-2019 and current 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1_top100 = ['https://www.tfrrs.org/lists/3191/2021_NCAA_Division_I_Outdoor_Qualifying/2021/o',\n",
    "             'https://www.tfrrs.org/lists/2909.html',\n",
    "             'https://www.tfrrs.org/archived_lists/2568/2019_NCAA_Division_I_Outdoor_Qualifying_(FINAL)/2019/o',\n",
    "             'https://www.tfrrs.org/archived_lists/2279/2018_NCAA_Division_I_Outdoor_Qualifying_(FINAL)/2018/o',\n",
    "             'https://www.tfrrs.org/archived_lists/1912/2017_NCAA_Div._I_Outdoor_Qualifying_(FINAL)/2017/o',\n",
    "             'https://www.tfrrs.org/archived_lists/1688/2016_NCAA_Division_I_Outdoor_Qualifying_(FINAL)/2016/o',\n",
    "             'https://www.tfrrs.org/archived_lists/1439/2015_NCAA_Division_I_Outdoor_Qualifying_(FINAL)/2015/o',\n",
    "             'https://www.tfrrs.org/archived_lists/1228/2014_NCAA_Division_I_Outdoor_Qualifying_(FINAL)/2014/o',\n",
    "             'https://www.tfrrs.org/archived_lists/1029/2013_NCAA_Division_I_Outdoor_Qualifying_(FINAL)/2013/o',\n",
    "             'https://www.tfrrs.org/archived_lists/840/2012_NCAA_Div._I_Outdoor_Qualifiers_(Final)/2012/o',\n",
    "             'https://www.tfrrs.org/archived_lists/673/2011_NCAA_Division_I_Outdoor_POP_List_(FINAL)/2011/o']\n",
    "\n",
    "D2_top100 = ['https://www.tfrrs.org/lists/3194/2021_NCAA_Division_II_Outdoor_Qualifying',\n",
    "             'https://www.tfrrs.org/lists/2908/2020_NCAA_Div._II_Outdoor_Qualifying/2020/o?gender=m',\n",
    "             'https://www.tfrrs.org/archived_lists/2571/2019_NCAA_Div._II_Outdoor_Qualifying_(FINAL)/2019/o',\n",
    "             'https://www.tfrrs.org/lists/2282.html',\n",
    "             'https://www.tfrrs.org/lists/1913.html',\n",
    "             'https://www.tfrrs.org/lists/1685.html',\n",
    "             'https://www.tfrrs.org/lists/1442.html',\n",
    "             'https://www.tfrrs.org/lists/1231.html',\n",
    "             'https://www.tfrrs.org/lists/1032.html',\n",
    "             'https://www.tfrrs.org/lists/841.html',\n",
    "             'https://www.tfrrs.org/lists/674.html',\n",
    "             'https://www.tfrrs.org/lists/529.html']\n",
    "\n",
    "D3_top100 = ['https://www.tfrrs.org/lists/3195/2021_NCAA_Division_III_Outdoor_Qualifying/2021/o',\n",
    "             'https://www.tfrrs.org/lists/2907/2020_NCAA_Div._III_Outdoor_Qualifying/2020/o?gender=m',\n",
    "             'https://www.tfrrs.org/lists/2572.html',\n",
    "             'https://www.tfrrs.org/lists/2283.html',\n",
    "             'https://www.tfrrs.org/lists/1914.html',\n",
    "             'https://www.tfrrs.org/lists/1684.html',\n",
    "             'https://www.tfrrs.org/lists/1443.html',\n",
    "             'https://www.tfrrs.org/lists/1232.html',\n",
    "             'https://www.tfrrs.org/lists/1033.html',\n",
    "             'https://www.tfrrs.org/lists/842.html',\n",
    "             'https://www.tfrrs.org/lists/675.html',\n",
    "             'https://www.tfrrs.org/lists/530.html']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Scrape! Apply the scrape function to the Division I, II and III lists of urls as well as the DI 2010 edge case that we already cleaned.  This will take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d1_scraped = scrape_tfrrs(D1_top100, 'D1')\n",
    "\n",
    "# add DI 2010 to d1_scraped\n",
    "d1_scraped = d1_scraped.append(pd.concat(dfs_copy2010))\n",
    "\n",
    "d1_scraped.index = range(1, len(d1_scraped) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d2_scraped = scrape_tfrrs(D2_top100, 'D2')\n",
    "d2_scraped.index = range(1, len(d2_scraped) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d3_scraped = scrape_tfrrs(D3_top100, 'D3')\n",
    "d3_scraped.index = range(1, len(d3_scraped) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to turn the individual lists of scraped and cleaned dataframes into one big dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list = [d1_scraped, d2_scraped, d3_scraped]\n",
    "\n",
    "tfrrs_scraped = pd.concat(big_list)\n",
    "tfrrs_scraped.index = range(1, len(tfrrs_scraped) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrrs_scraped.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we can output individual csv files for each division and the combined scrape.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_scraped.to_csv('d1top100.csv', index=False)\n",
    "\n",
    "d2_scraped.to_csv('d2top100.csv', index=False)\n",
    "\n",
    "d3_scraped.to_csv('d3top100.csv', index=False)\n",
    "\n",
    "tfrrs_scraped.to_csv('tfrrstop100.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voila! Now we have some data to work with.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
